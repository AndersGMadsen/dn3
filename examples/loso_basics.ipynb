{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Basic LOSO\n",
    "----------\n",
    "This covers a fundamental use-case for DN<sup>3</sup>. Here, the TIDNet architecture from *Kostas & Rudzicz 2020\n",
    "(under review)* is evaluated using the BCI competition IV dataset 2a four-class motor imagery dataset in a\n",
    "*\"leave one subject out\"* training procedure. This means, for each person in the dataset, the remaining subjects are\n",
    "used to develop a classifier, and performance is measured using\n",
    "\n",
    "Here we assume that the original *.gdf* files that comprise the dataset were\n",
    "[downloaded from the original source](<http://www.bbci.de/competition/iv/>), then were organized into a\n",
    "subdirectory per person containing the *T* (training) and *E* (evaluation) sessions. In this common format, we can use\n",
    "the following configuration file (let's call it `bci_iv_config.yml`) to easily import the dataset\n",
    "\n",
    "```yaml\n",
    "DN3:\n",
    "  datasets:\n",
    "    - bci_2a\n",
    "  epochs: 5\n",
    "  batch_size: 60\n",
    "\n",
    "bci_2a:\n",
    "  name: \"BCI Competition IV - dataset 2a\"\n",
    "  toplevel: /path/to/the/toplevel/folder\n",
    "  tmin: -0.5\n",
    "  tlen: 4.5\n",
    "  picks:\n",
    "    - eeg\n",
    "    - eog\n",
    "  events:\n",
    "    left_hand: 769\n",
    "    right_hand: 770\n",
    "    foot: 771\n",
    "    tongue: 772\n",
    "\n",
    "```\n",
    "\n",
    "Notice that in addition to the dataset, we include some general experimental configuration parameters in the DN3\n",
    "section. These are *not mandatory*, its just a convenient place to keep track of variables like this, so that our code\n",
    "doesn't have to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from dn3.configuratron import ExperimentConfig\n",
    "from dn3.trainable.processes import StandardClassification\n",
    "from dn3.trainable.models import TIDNet\n",
    "\n",
    "# Since we are doing a lot of loading, this is nice to suppress some tedious information\n",
    "import mne\n",
    "mne.set_log_level(False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First things first, we use `ExperimentConfig`, and the subsequently constructed `DatasetConfig` to rapidly construct\n",
    "our `dataset`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bci_iv_config.yml'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-fb11cc930580>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mconfig_filename\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'bci_iv_config.yml'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0muse_gpu\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mexperiment\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mExperimentConfig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig_filename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mexperiment\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdatasets\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'bci_2a'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Thesis/DNPT/dn3/configuratron/config.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, config_filename, adopt_auxiliaries)\u001B[0m\n\u001B[1;32m     37\u001B[0m                              \u001B[0mobject\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mlater\u001B[0m \u001B[0muse\u001B[0m\u001B[0;34m.\u001B[0m \u001B[0mDefaults\u001B[0m \u001B[0mto\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m.\u001B[0m \u001B[0mThis\u001B[0m \u001B[0mwill\u001B[0m \u001B[0mpropagate\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mdetected\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m         \"\"\"\n\u001B[0;32m---> 39\u001B[0;31m         \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig_filename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'r'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mfio\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_original_config\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0myaml\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfull_load\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfio\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0mworking_config\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_original_config\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'bci_iv_config.yml'"
     ]
    }
   ],
   "source": [
    "config_filename = 'bci_iv_config.yml'\n",
    "use_gpu = False\n",
    "experiment = ExperimentConfig(config_filename)\n",
    "dataset = experiment.datasets['bci_2a']\n",
    "\n",
    "dataset = dataset.auto_construct_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can simply use the `.loso()` method to loop over each person in the dataset. Good practice would see that any\n",
    "validation data (data held out from the *training set* that is evaluated to track sensitivity to the training data\n",
    "alone), is a similarly set aside person. So the default in DN<sup>3</sup> is to provide a unique validation\n",
    "person for each testing person.\n",
    "\n",
    "Let's create a function that makes a new model for each set of training people and a trainable process for\n",
    "`StandardClassification`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_model_and_process():\n",
    "    tidnet = TIDNet.from_dataset(dataset)\n",
    "    return StandardClassification(tidnet)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's it! We use a helper that initializes a TIDNet from any `Dataset/Thinker/EpochedRecording` and wrap it with a\n",
    "`StandardClassification` process. Invoking this process will train the classifier. Looking through `trainable.processes`\n",
    "will show you what other processes can be invoked (perhaps on the same model, e.g. if you wanted to fine-tune this\n",
    "model later).\n",
    "\n",
    "This is really all you need, with a handful of lines of code, state-of-the-art results. Just loop through the `.loso()`\n",
    "method, fit the classifier, then check the test results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = dict()\n",
    "for training, validation_thinker, test_thinker in dataset.loso():\n",
    "    process = make_model_and_process()\n",
    "\n",
    "    process.fit(training_dataset=training, validation_dataset=validation_thinker, epochs=experiment.epochs, batch_size=experiment.batch_size)\n",
    "\n",
    "    performance = process.evaluate(test_thinker)\n",
    "    results[test_thinker.person_id] = performance\n",
    "\n",
    "print(results)\n",
    "print(\"Average accuracy: {:.2%}\".format(sum(results.values())/len(results)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}